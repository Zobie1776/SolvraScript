# SolvraScript Optimization Recommendations - 2025-11-29

This document provides specific, actionable optimization recommendations for the SolvraScript frontend and its associated tooling.

---

## 1. Parser & AST

- **Recommendation:** Implement a memoization layer in the parser for module imports. When the same module is imported multiple times across a project, the parsing step should only be performed once. The resulting AST can be cached and reused.
- **Justification:** In large projects, the same core modules are often imported repeatedly. Re-parsing them every time is a waste of CPU cycles and I/O. Caching the AST is a standard and highly effective optimization.
- **Expected Gain:** Significant improvement in compilation times for large, multi-module projects.
- **Risk:** Low. The main risk is cache invalidation. The caching mechanism must be able to detect if the source file has changed on disk since it was last parsed. A simple timestamp or file hash check would be sufficient.

- **Recommendation:** Pre-calculate and store layout information (e.g., memory size and field offsets) for all `struct` and `class` definitions directly on the AST nodes during a dedicated analysis pass.
- **Justification:** This information is static and will be needed by the backend (VM/JIT) for memory allocation and property access. Calculating it once, upfront, prevents the backend from having to repeatedly compute it. This simplifies the backend and moves a fixed cost to the compile-time stage.
- **Expected Gain:** Faster JIT compilation and more efficient property access in the interpreter.
- **Risk:** Low. This is a standard compiler optimization.

---

## 2. Module Resolver

- **Recommendation:** Enhance the module resolver's caching strategy. The current resolver should be audited to ensure it has a robust, multi-layered cache.
    1.  **In-Memory Cache:** An in-memory `HashMap` that maps module paths to resolved file paths.
    2.  **On-Disk Cache:** A persistent on-disk cache (e.g., in a `.solvra_cache` directory) that stores the resolution results between compiler runs.
- **Justification:** Module resolution can be a major bottleneck, involving many filesystem lookups. A multi-level cache will dramatically reduce this I/O overhead. An in-memory cache serves a single compile session, while a persistent disk cache speeds up all subsequent compilations.
- **Expected Gain:** Drastic reduction in compile times, especially for projects with many dependencies.
- **Risk:** Medium. Cache invalidation is the primary risk. The on-disk cache, in particular, must be intelligently invalidated if the project's dependency graph changes (e.g., a `package.json` equivalent is modified) or if a module's location on disk changes.

---

## 3. IR & Bytecode Generation

- **Recommendation:** Design the bytecode (`.svc` format) to be as "backend-agnostic" as possible. It should be a simple, linear instruction set that makes minimal assumptions about the underlying execution engine. Avoid including complex, high-level constructs in the bytecode itself.
- **Justification:** A simple bytecode format makes it much easier to write and debug the backends (interpreter, JITs). The lowering process from AST to bytecode should handle the complexity. This keeps the Tier-0 interpreter simple and fast, and provides a stable, easy-to-consume input for the Tier-1 JIT.
- **Expected Gain:** Faster development of backend features, improved debuggability, and a more stable compiler pipeline.
- **Risk:** Low. This is a core principle of good compiler design.

- **Recommendation:** Implement a peephole optimization pass over the generated bytecode.
- **Justification:** The initial bytecode generated from the AST is often inefficient. A simple peephole optimizer can scan for and replace common inefficient patterns, such as redundant `load`/`store` instructions or constant folding opportunities that were missed in the AST stage.
- **Expected Gain:** Moderate improvement in runtime performance for all execution tiers. This is a high-value, low-effort optimization.
- **Risk:** Low. The patterns for peephole optimization are well-understood.

---

## 4. What NOT to Change

- **Do NOT Over-optimize the Parser:** The parser is already well-structured. While memoization for module imports is good, avoid trying to micro-optimize the parsing of individual files. The cost of parsing is rarely the bottleneck compared to I/O and code generation.
- **Do NOT Change the AST Structure Drastically:** The AST is well-designed and has foresight for future features. Radical changes would require a full rewrite of the frontend. New node types can be added, but the core structure should be preserved.
- **Do NOT Break the Host Bridge Abstraction:** The security of the language depends on the clean separation between the high-level `stdlib` wrappers and the low-level native code. Do not introduce any mechanism that would allow script code to bypass the Host Bridge and call native functions directly.
